---
title: "Modeling"
author: "Hayden Morgan"
format: html
editor_options: 
  chunk_output_type: console
---

Before beginning: adding libraries so our code runs smoothly (messages and warnings silenced).
```{r, message=FALSE, warning=FALSE}
library("dplyr")
library("tidyverse")
library("tidymodels")
library("caret")
library("yardstick")
library("ranger")
```

## Basic Introduction
Again, for this project we are using a data set about diabetes health. The data comes from the CDC, and the set we are using is clean data. We are told that we have a binary response variable called Diabetes_binary, which tells us if the person does not have diabetes or has prediabetes/diabetes. There are 21 variables total, but for the purpose of this project we are only choosing 3 variables in addition to the response variable to focus on. I chose to focus on **physical activity (0 means no physical activity in the past 30 days, 1 means the person did engage in physical activity in the past 30 days), body mass index (continuous numeric variable),**, and **fruits (0 means the person does not consume fruit 1 or more times per day, 1 means the person consumes 1 or more fruits each day).**

In the EDA .qmd file, I explained why I chose these three variables specifically already (you can go back to the EDA page [here](EDA.html)). So here, I will talk a little bit about the difference between this Modeling file and the EDA file completed previously. Exploratory data analysis has already been performed so that we have a better understanding and vision of the data set overall. Now, attention is turned to modeling so that we can look at historical data and try to predict what similar data might look like in the future. There are many different types of models that can be selected for a data set, and this project will cover and explain 4 of them. The goal of this document is to choose the model that best fits the data set at hand (e.g. select the model that does the best job at predicting data in this context).

Here, I am presenting the clean/factored data set again (from the EDA.qmd file) for use in the Modeling.qmd file:
```{r}

diabetes <- read.csv("data/diabetes_binary_health_indicators_BRFSS2015.csv")

diabetes$response <- factor(diabetes$Diabetes_binary, levels = c(0, 1), labels = c("No Diabetes", "Has Prediabetes or Diabetes")) #response variable is Diabetes_binary

diabetes$HighBP <- factor(diabetes$HighBP, levels = c(0, 1), labels = c("No High BP", "High BP")) 

diabetes$HighChol <- factor(diabetes$HighChol, levels = c(0, 1), labels = c("No High Chol", "High Chol"))

diabetes$CholCheck <- factor(diabetes$CholCheck, levels = c(0, 1), labels = c("Does Not Check Cholesterol", "Does Check Cholesterol")) 

diabetes$Smoker <- factor(diabetes$Smoker, levels = c(0, 1), labels = c("Not Smoker", "Smoker"))

diabetes$Stroke <- factor(diabetes$Stroke, levels = c(0, 1), labels = c("No Stroke", "Has Had Stroke")) 

diabetes$HeartDiseaseorAttack <- factor(diabetes$HeartDiseaseorAttack, levels = c(0, 1), labels = c("No Heart Disease", "Has Had Heart Disease"))

diabetes$PhysActivity <- factor(diabetes$PhysActivity, levels = c(0, 1), labels = c("Does Not Exercise", "Exercises")) 

diabetes$Fruits <- factor(diabetes$Fruits, levels = c(0, 1), labels = c("Does Not Eat Fruit", "Eats Fruit")) 

diabetes$Veggies <- factor(diabetes$Veggies, levels = c(0, 1), labels = c("Does Not Eat Veggies", "Eats Veggies")) 

diabetes$HvyAlcoholConsump <- factor(diabetes$HvyAlcoholConsump, levels = c(0, 1), labels = c("Not A Heavy Drinker", "Heavy Drinker")) 

diabetes$AnyHealthcare <- factor(diabetes$AnyHealthcare, levels = c(0, 1), labels = c("No Healthcare", "Has Healthcare")) 

diabetes$NoDocbcCost <- factor(diabetes$NoDocbcCost, levels = c(0, 1), labels = c("No Cost Barrier In Seeing Doctor", "Cost Barrier In Seeing Doctor")) 

diabetes$GenHlth <- factor(diabetes$GenHlth, levels = c(1:5), labels = c("Excellent Health", "Very Good Health", "Good Health", "Fair Health", "Poor Health")) 

diabetes$DiffWalk <- factor(diabetes$DiffWalk, levels = c(0, 1), labels = c("No Difficulty Walking", "Has Difficulty Walking")) 

diabetes$Sex <- factor(diabetes$Sex, levels = c(0, 1), labels = c("Female", "Male"))

diabetes$Age <- factor(diabetes$Age, levels = c(1:13), labels = c("18 - 24", "25 - 29", "30 - 34", "35 - 39", "40 - 44", "45 - 49", "50 - 54", "55 - 59", "60 - 64", "65 - 69", "70 - 74", "75 - 79", "80+")) 

diabetes$Education <- factor(diabetes$Education, levels = c(1:6), labels = c("No School/K Only", "Elementary School", "Middle School", "High School", "College", "Graduate Education")) 

diabetes$Income <- factor(diabetes$Income, levels = c(1:8), labels = c("Less than $10,000", "$10,000 - less than $15,000", "$15,000 - less than $20,000", "$20,000 - less than $25,000", "$25,000 - less than $35,000", "$35,000 - less than $50,000", "$50,000 - less than $75,000", "More than $75,000"))

```

To train a model but not make it wholly dependent on data it has already seen, we will split the data we do have into training and test data. The model will be trained on the training data, and then its performance will be evaluated on the test data. We use set.seed() first to produce reproducible random numbers for the purposes of model training/testing. 
```{r}

set.seed(100) #chose 100 because it is a similar value to our HW5 assignment 

split <- initial_split(diabetes, prop = 0.7) #project instructions specify 70/30 split 
train <- training(split)
test <- testing(split)

folds <- vfold_cv(train, 5) #project instructions specify 5 folds  

```

Again, the ultimate goal of this Modeling file is to create models for predicting diabetes status and then select the model that is best at predicting. Per project instructions, it's ok to use the caret package. We are using logLoss as the metric for evaluation. All model types will use logLoss with a 5 fold cross-validation, and grids of tuning parameters are to be set up where possible. 

## Logistic Regression Models

Logistic regression models are indicated for use when the response variable is binary and there is a classification task at hand. For our data, this is indeed the case. With a logistic regression model, we aim to predict the probability that the response is either 0 (does not have diabetes) or 1 (has prediabetes/diabetes). Logistic regression models employ log-odds instead of modeling the response directly (e.g. like linear regression does).

Of note, the project instructions say "You should set up your own grid of tuning parameters in any model where that is possible." This logistic regression model will be the only model where this is NOT possible.
```{r}

#creating the recipe for each of our 3 log reg models, as specified in project instructions 
log_phys <- recipe(response ~ PhysActivity + BMI, data = train) |>
  step_normalize(BMI) #normalize numeric variables; for logreg, it's unnecessary to do step_dummy()
log_bmi <- recipe(response ~ BMI, data = train) |>
  step_normalize(BMI)
log_fruits <- recipe(response ~ Fruits + BMI, data = train) |>
  step_normalize(BMI)

spec_log <- logistic_reg() |>
  set_engine("glm") 

#setting the workflow for each model we want to test 
logphys_wfl <- workflow() |>
  add_recipe(log_phys) |>
  add_model(spec_log)
logbmi_wfl <- workflow() |>
  add_recipe(log_bmi) |>
  add_model(spec_log)
logfruits_wfl <- workflow() |>
  add_recipe(log_fruits) |>
  add_model(spec_log)

metrics <- metric_set(mn_log_loss)

#using the folds we already created to fit the models
logphys_fit <- logphys_wfl |>
  fit_resamples(folds, metrics = metrics)
logbmi_fit <- logbmi_wfl |>
  fit_resamples(folds, metrics = metrics)
logfruits_fit <- logfruits_wfl |>
  fit_resamples(folds, metrics = metrics)

#viewing together the metrics (log loss) for all our models in order to determine the best model
rbind(logphys_fit |> collect_metrics(),
      logbmi_fit |> collect_metrics(),
      logfruits_fit |> collect_metrics()) |>
  mutate(Model = c("Physical Activity + BMI Model", "BMI Only Model", "Fruit Consumption + BMI Model")) |>
  select(Model, everything())

```

The best model using cross-validation and log loss as the metric includes predictors of both physical activity and BMI (smallest log loss).

## Classification Tree

Classification tree models are also used when the response variable is categorical, though in this case it doesn't necessarily have to be binary (unlike logistic regression). They are useful in classification tasks and are able to split into branches based on explanatory variables. For example, with our variable about fruit consumption status, there would be separate branches for "eats fruit - yes" and "eats fruit - no". At the end of classification, the prediction is chosen based on which answer is the majority.

Of note, the logistic regression model instructions above prompted us to fit three different models and choose from the three. For this task, we are only fitting ONE classification tree that has varying values for the complexity parameter - then choosing the best model. So there will not be three separate models for this section. 
```{r}

#again, only using one model here (I chose the model that performed the best in the logreg section) but altering the complexity parameter later on
tree <- recipe(response ~ PhysActivity + BMI, data = train) |>
  step_dummy(PhysActivity) |> #do need step_dummy() for this categorical variable
  step_normalize(BMI)

tree_spec <- decision_tree(tree_depth = 5, 
                              min_n = 10, 
                              cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

tree_wfl <- workflow() |>
  add_recipe(tree) |>
  add_model(tree_spec)

metrics <- metric_set(mn_log_loss)

#this is how I'm putting different values for cost complexity per the project instructions
grid <- tibble(cost_complexity = 10^seq(-4, -1, length.out = 10))

tune <- tune_grid(
  tree_wfl,
  resamples = folds, 
  grid = grid,
  metrics = metrics
)

#this should select the best model based on cost complexity 
best_tree <- select_best(tune, metric = "mn_log_loss")

tree_final_wfl <- tree_wfl |>
  finalize_workflow(best_tree)

tree_final_fit <- tree_final_wfl |>
  last_fit(split, metrics = metrics)

#we only have one model for this section, and we're able to see the resulting log less metric 
tree_final_fit |> collect_metrics()


```

The best model using cross-validation and log loss has a complexity parameter of 0.0001. The log loss is 0.404. 

## Random Forest

Random forest models take the idea of classification trees one step farther by relying on multiple trees as well as bootstrapped samples to make a prediction. The average result will serve as a final prediction: majority for categorical or mean for numerical. One key to random forest models is that splits occur based on a random subset of predictors each time - the number of predictors is the tuning parameter. In other words, random forests do not use every single predictor at each step. Bootstrapping is key here in order to reduce overfitting and improve the ability for the model to generalize in the context of random forest models. Bootstrapping means that the training data being used for the model will be sampled with replacement in order to get more mileage out of the training data we have. This means that every tree in the forest will be trained with a slightly different version of the training data. The overall ideal is that if there is a very strong predictor, each bootstrapped tree will likely use it for the first split.
```{r}

#again here, making recipes with step_dummy() and step_normalize
rtree_phys <- recipe(response ~ PhysActivity + BMI, data = train) |>
  step_dummy(PhysActivity) |>
  step_normalize(BMI)
rtree_bmi <- recipe(response ~ BMI, data = train) |>
  step_normalize(BMI) 
rtree_fruits <- recipe(response ~ Fruits + BMI, data = train) |>
  step_dummy(Fruits) |>
  step_normalize(BMI)

rtree_spec <- rand_forest(mtry = tune()) |>
  set_engine("ranger") |>
  set_mode("classification")

#making workflows for the differnet models 
rtreephys_wfl <- workflow() |>
  add_recipe(rtree_phys) |>
  add_model(rtree_spec)
rtreebmi_wfl <- workflow() |>
  add_recipe(rtree_bmi) |>
  add_model(rtree_spec)
rtreefruits_wfl <- workflow() |>
  add_recipe(rtree_fruits) |>
  add_model(rtree_spec)

metrics <- metric_set(mn_log_loss)

#using folds for runing for the models 
rtreephys_fit <- rtreephys_wfl |>
  tune_grid(resamples = folds, metrics = metrics)
rtreebmi_fit <- rtreebmi_wfl |>
  tune_grid(resamples = folds, metrics = metrics)
rtreefruits_fit <- rtreefruits_wfl |>
  tune_grid(resamples = folds, metrics = metrics)

#choosing the best of each model 
best_rphys <- select_best(rtreephys_fit, metric = "mn_log_loss")
best_rbmi <- select_best(rtreebmi_fit, metric = "mn_log_loss")
best_rfruits <- select_best(rtreefruits_fit, metric = "mn_log_loss")

rphys_final_wfl <- rtreephys_wfl |>
  finalize_workflow(best_rphys)
rbmi_final_wfl <- rtreebmi_wfl |>
  finalize_workflow(best_rbmi)
rfruits_final_wfl <- rtreefruits_wfl |>
  finalize_workflow(best_rfruits)

rphys_final_fit <- rphys_final_wfl |>
  last_fit(split, metrics = metrics)
rbmi_final_fit <- rbmi_final_wfl |>
  last_fit(split, metrics = metrics)
rfruits_final_fit <- rfruits_final_wfl |>
  last_fit(split, metrics = metrics)

#putting the best model fits together to compare them side by side based on log loss metric
bind_rows(rphys_final_fit |> collect_metrics() |> mutate(Model = "Physical Activity + BMI Model"),
      rbmi_final_fit |> collect_metrics() |> mutate(Model = "BMI Only Model"),
      rfruits_final_fit |> collect_metrics() |> mutate(Model = "Fruit Consumption + BMI Model")) |>
  filter(.metric == "mn_log_loss") |>
  select(Model, Log_Loss = .estimate)

```

The best model using cross-validation and log loss as the metric includes predictors of both physical activity and BMI (smallest log loss).

## Final Model Selection

Now that we have identifie dthe best model from the three model types above, we will compare the models to each other based on the log loss metric to pick the best model out of the three types.
```{r}

#the logreg model is the only one that hasn't undergone a last fit yet, so doing that now 
logbmi_final_fit <- logphys_wfl |>
  last_fit(split, metrics = metric_set(mn_log_loss))

#putting all three models together to view side by side 
bind_rows(logbmi_final_fit |> collect_metrics() |> mutate(Model = "Logistic Regression Model"),
      tree_final_fit |> collect_metrics() |> mutate(Model = "Classification Tree Model"),
      rphys_final_fit |> collect_metrics() |> mutate(Model = "Random Forest Model")) |>
  filter(.metric == "mn_log_loss") |>
  select(Model, Log_Loss = .estimate)

```

The best model using cross-validation and log loss as the metric is the Random Forest Physical Activity + BMI model (smallest log loss).
